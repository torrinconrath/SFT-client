Current Model Setup:

llm = LLM(
    model=model_path,
    gpu_memory_utilization=0.75,
    trust_remote_code=True,
    max_model_len=4096,
    dtype="float16",
    enable_chunked_prefill=True,
    max_num_batched_tokens=4096,
    quantization="bitsandbytes",
)

# Default sampling parameters
sampling_params = SamplingParams(
    temperature=0.3,
    max_tokens=1024,
    top_p=0.9,
    skip_special_tokens=True,
)